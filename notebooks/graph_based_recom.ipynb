{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('deneme': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ee72a30c5f6d113551513725604adea714e893713f4c0ab7fc96f08bddff6bdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path().cwd().parent\n",
    "DATA_DIR = PROJECT_DIR / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessData():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read_file(self, file: str, file_type: str):\n",
    "        \"\"\"Takes a filename(excel or csv), Returns a pandas dataframe object.\"\"\"\n",
    "        if file_type == \"excel\":\n",
    "            df = pd.read_excel(file)\n",
    "        elif file_type == \"csv\":\n",
    "            df = pd.read_csv(file)\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def get_user_and_items(self, dataframe, user_col: str, item_col: str):\n",
    "        \"\"\"Takes a dataframe containing user-item interactions, Returns item list purchased by each user.\"\"\"\n",
    "        user_and_items = {}\n",
    "        for name, group in df.loc[:,[user_col, item_col]].groupby(by=user_col):\n",
    "            user_and_items.update({name:group.loc[:,item_col].to_list()})\n",
    "\n",
    "        return user_and_items\n",
    "\n",
    "    def transform_text_data(self, corpus: list):\n",
    "        \"\"\"Takes a list of tokens, Returns tf-idf vectors.\"\"\"\n",
    "        pipe = Pipeline([\n",
    "                    (\"count\", CountVectorizer(\n",
    "                                ngram_range=(1,1), \n",
    "                                token_pattern=r\"[a-zA-Z0-9ıIiİğĞçÇöÖüÜşŞ]+\", \n",
    "                                strip_accents=\"unicode\",\n",
    "                                lowercase=True)),\n",
    "                    (\"tfidf\", TfidfTransformer())\n",
    "                ])\n",
    "        \n",
    "        return pipe.fit_transform(corpus)\n",
    "\n",
    "    def compute_similarity(self, first: list, second: list):\n",
    "        \"\"\"Takes two array of numbers, Returns cosine similarity score between these two arrays.\"\"\"\n",
    "        from numpy.linalg import norm\n",
    "        similarity_score = np.dot(first, second)/(norm(first)*norm(second))\n",
    "        # from sklearn.metrics.pairwise import cosine_similarity\n",
    "        # cosine_similarity(first.reshape(1,-1), second.reshape(1,-1))\n",
    "        return similarity_score\n",
    "\n",
    "processor = ProcessData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"generic_rec_dataset_Ekim_New_data.xlsx\"\n",
    "file_path = DATA_DIR / file_name\n",
    "df = processor.read_file(file_path, file_type=\"excel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Users: 97\nNumber of items: 465\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "# df.loc[:, [\"USER\", \"ITEM\"]].groupby(by=\"USER\").count().sort_values(by=\"ITEM\", ascending=False).reset_index()\n",
    "user_ids = df.loc[:, \"USER\"].unique()\n",
    "item_ids = df.loc[:, \"ITEM\"].unique()\n",
    "print(\"Number of Users: {users}\\nNumber of items: {items}\".format(users=len(user_ids), items=len(item_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_and_items = processor.get_user_and_items(df, \"USER\", \"ITEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'İş Modelinizi Yeniden Düşünün',\n",
    "    'Dijital Dünyada Müşteri Merkezinde Kalmak',\n",
    "    'Dijital Dönüşümün Sektörler Üzerinde Etkileri - Video',\n",
    "    'Dijital Dönüşümün Kurumlara Etkisi - Okuma Materyali - vİdeo',\n",
    "    'KVKK Uyum Programı İçin Hazır Mısınız? için',\n",
    "    'Detaylarıyla KiŞisel Verilerin Korunması Kanunu',\n",
    "    'İK Departmanları İçin Kişisel Verilerin Korunması Kanunu',\n",
    "    'İSG 360° Eğitim Programı - 6331 Sayılı İş Sağlığı ve Güvenliği Kanunu IĞDIR',\n",
    "]\n",
    "# vec = CountVectorizer(ngram_range=(1,1), token_pattern=r\"[a-zA-Z0-9ıIiİğĞçÇöÖüÜşŞ]+\", strip_accents=\"unicode\", lowercase=True)\n",
    "# x = vec.fit_transform(corpus)\n",
    "# vec.get_feature_names()\n",
    "# vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.35668304960107816"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "y = processor.transform_text_data(corpus)\n",
    "similarity_score = processor.compute_similarity(y.toarray()[2], y.toarray()[3])\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3779, 3780)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "903"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "from itertools import permutations, combinations\n",
    "item_pairs = []\n",
    "user_pairs = []\n",
    "for items in user_and_items.values():\n",
    "    item_pairs.extend([item_pair for item_pair in combinations(items, 2)])\n",
    "    break\n",
    "\n",
    "for item_pair in item_pairs:\n",
    "    print(item_pair)\n",
    "    first\n",
    "    break\n",
    "len(item_pairs)\n",
    "# len([a for a in combinations(b, 2)])\n",
    "# [permutations(item) for item in user_and_items.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "a = [(1,2,{\"weight\":0.14}),(2,1,{\"weight\":0.14}),(1,2,{\"weight\":0.14}),(1,3,{\"weight\":0.44}),(3,1,{\"weight\":0.44}),(2,3,{\"weight\":0.90}), (1,4,{\"weight\":0.90}), (2,4,{\"weight\":0.20})]\n",
    "\n",
    "G = networkx.Graph()\n",
    "G.add_edges_from(a)\n",
    "# G.edges.data('weight')\n",
    "[x for x in networkx.shortest_path(G, source=1, target=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EdgeView([(1, 2), (1, 3), (1, 4), (2, 3), (2, 4)])"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "G.edges"
   ]
  }
 ]
}